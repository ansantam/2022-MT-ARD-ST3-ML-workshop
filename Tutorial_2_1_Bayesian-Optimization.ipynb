{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Bayesian Optimization\n",
    "\n",
    "In this tutorial notebook we'll implement all the basic components of Bayesian optimization (BO), and see how to use BO for some sample functions.\n",
    "\n",
    "## 0. Glossary and General Introduction\n",
    "\n",
    "### 0.1 Bayes' Theorem\n",
    "\n",
    "First, let's start with the absolute basics of Bayesian inference, the Bayes' theorem\n",
    "    $$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "- $P(A|B)$ is the _posterior probablity_ of event $A$ given that event B is observed.\n",
    "- $P(B|A)$ is the _likelihood function_ of A given B; it is also the conditional probablity of observing $B$ given $A$.\n",
    "- $P(A), P(B)$ are the _prior probabilities_ of observing $A$ and $B$, also known as _marginal probability_\n",
    "\n",
    "For most applications, the observed event $B$ is already fixed and we can consider $P(B)$ simply as a constant. Thus the Bayes' theorem reads\n",
    "    $$P(A|B) \\propto P(B|A) P(A) $$,\n",
    "i.e. the posterior probability is proportional to priors times likelihood.\n",
    "\n",
    "### 0.2 Gaussian Process\n",
    "\n",
    "A Gaussian process (GP) is a stochastic process (a joint distribution of infinitely many random variables), which can be used as a probablistic model (surrogate model) for the objective function in regression and classification tasks.\n",
    "\n",
    "A GP can be fully described by it's mean $\\mu$ and covariance function $k(\\cdot,\\cdot)$\n",
    "    $$f(x) \\sim \\mathcal{GP}(\\mu(x), k(x,x'))$$\n",
    "\n",
    "- __Gaussian process regression (GPR)__, also formerly known as _kriging_: a method to interpolate a unkown objective function.\n",
    "- __Prior mean__ $\\mu(x)$: basic building block of GP; prior belief on the averaged objective function values, usually set to a constant if the function behaivor is unknown.\n",
    "- __Kernel__ $k(\\cdot,\\cdot)$, also known as the _covariance function_ $cov(\\cdot,\\cdot)$: basic building block of GP; prior belief on the characteristics of the unkown function.\n",
    "\n",
    "#### 0.2.1 Common Kernels used in GP\n",
    "\n",
    "Some of the commonly used kernels are listed below. They can also be combined to build more complex kernels representing the underlying physics of the objective function.\n",
    "\n",
    "- Linear: $k_\\mathrm{L}(x,x') = x^\\intercal x'$\n",
    "- __White Gaussian noise__: $k_\\mathrm{n}(x,x')=\\sigma_{n}^2 \\delta_{x,x'}$ , i.e. a diagnal noise term with Gaussian noise $\\sigma_{n}^2$.\n",
    "- __Radial basis function (RBF)__, also known as squared exponential (SE): $k_{\\mathrm{RBF}} (x,x') = \\exp(-\\frac{||x-x'||^2}{2 l^2})$ , where $l$ is the length scale, see below. This resembles a normal Gaussian distribution.\n",
    "- Matérn: $k_\\mathrm{Matern} (x,x') = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\sqrt{2\\nu ||x-x'||} / l \\right)^\\nu K_\\nu \\left( \\sqrt{2\\nu ||x-x'||} / l \\right) $, where $\\Gamma$ is the gamma function, $K_\\nu$ the modified Bessel function and $\\nu$ the parameter of the Matérn kernel. Common choices are $\\nu=\\frac{3}{2},\\frac{5}{2},...$\n",
    "- Ornstein-Uhlenbeck: $k_\\mathrm{OU}(x,x')=\\exp(-||x-x'||/l)$\n",
    "\n",
    "\n",
    "#### 0.2.2 GP Hyperparamters \n",
    "\n",
    "The characteristics of GP, or its ability to approximate the unknown function are dependent both on __the choice of the covariance function__ and the __values of the hyperparamters__. The hyperparamters are usually either choosen manually based on the physics, or obtained from the maximum likelihood fit (log-likelihood fit, maximum a posteriori fit) during the optimization.\n",
    "\n",
    "- __Lengthscale__ $l$:\n",
    "\n",
    "\n",
    "### 0.3 Bayesian Optimization\n",
    "\n",
    "Below are some general terms used in the BO field, some of which are often used interchangeably.\n",
    "\n",
    "- __Acquisition function__ $\\alpha$: is built on the GP posterior, controls the behavior of optimization. For the standard verison of BO, the next sample point is chosen at $\\mathrm{argmax}(x)$. In this tutorial we will introduce two widely used acquisition functions: the expected improvment (EI) and the upper confidence bound (UCB).\n",
    "- __Objective__, metric, or target function: a unknown (black-box) function, for which the value is to be optimized (here: maximization).\n",
    "- __Search space__, bounds, or optimization range: A (continous) parameter space where the input parameters are allowed to be varied in the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF, DotProduct, WhiteKernel\n",
    "from sklearn.datasets import make_friedman2\n",
    "from skopt import gp_minimize\n",
    "from scipy.optimize import minimize\n",
    "from utils.gp_helper import plot_gp, plot_gp_with_acq, plot_bo_result, Acquisition, AcqEI, AcqUCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a Gaussian Process\n",
    "\n",
    "Here we will use the [scikit-learn](https://scikit-learn.org/stable/modules/gaussian_process.html) to build a Gaussian process model.\n",
    "In the end of this notebook you will also find a incomplete set of other commonly used GP and BO projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a RBF kernel (covariance function)\n",
    "kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "# Initialize a GP model\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define an Unkown Target Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_target(x, noise=0.1, rng = np.random.default_rng()):\n",
    "    f = (-1.4 + 3.0 * x) * np.sin(18.0 * x)\n",
    "    return f + rng.random(x.shape) * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize it\n",
    "xlow, xhigh = 0, 1\n",
    "x = np.linspace(xlow,xhigh,200)\n",
    "y = f_target(x, noise=0)\n",
    "x_samples = rng.uniform(low=xlow,high=xhigh,size=6)\n",
    "y_samples = f_target(x_samples, noise=0.1, rng=rng)\n",
    "\n",
    "plt.plot(x,y,label='True f')\n",
    "plt.plot(x_samples,y_samples,\"*\",label=\"Noisy Samples\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to change the length_scale below to see the difference in resulted GP posteriors.\n",
    "\n",
    "One can use `kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))` to also fit the length_scale within the specified range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define GP model\n",
    "kernel = 1 * RBF(length_scale=0.1, length_scale_bounds=\"fixed\") + WhiteKernel(noise_level=0.1)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the GP fitted to the target function\n",
    "fig, axes = plt.subplots(1,2, figsize=(8,3))\n",
    "ax1, ax2 = axes\n",
    "\n",
    "# plot GP prior\n",
    "ax1.set_title(\"Prior\")\n",
    "plot_gp(gpr, x, y, x_samples, y_samples, ax=ax1)\n",
    "\n",
    "# fit GP to the function and plot posterior2\n",
    "gpr.fit(x_samples.reshape(-1,1),y_samples)\n",
    "ax2.set_title(\"Posterior\")\n",
    "plot_gp(gpr, x, y, x_samples, y_samples, ax=ax2)\n",
    "\n",
    "fig.tight_layout()\n",
    "ax1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Maximization of the acquisition function\n",
    "\n",
    "Here we will implement two acquisition functions: \n",
    "\n",
    "the __expected improvement (EI)__\n",
    "$$ \\begin{split}   \n",
    "\\alpha_{\\text{EI}} (x)  &= \\mathbb{E} [ \\max (\\mu (x)-(f_{\\text{best}}+\\xi),0)  ] \\\\  \n",
    "&= (\\mu(x)-(f_{\\text{best}}+\\xi)) \\Phi (Z) + \\sigma (x)\\phi (Z) \n",
    "\\end{split}, \n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{split}\n",
    "         Z  & = \\frac{\\mu(x)-(f_{\\text{best}}+\\xi)}{\\sigma(x)},\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "and the __upper confidence bound (UCB)__\n",
    "$$\n",
    "    \\alpha_{\\text{UCB}} (x) = \\mu(x) + \\kappa \\sigma (x).\n",
    "$$\n",
    "\n",
    "See `utils/gp_helper.py` for the implementation of the acquisition fucntions.\n",
    "\n",
    "__Exploration-exploitation tradeoff__: the behaviour of the acquisition fucntions are controlled via the hyperparameters $\\xi$ and $\\kappa$. Larger values lead to more exploration and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a EI acquisition\n",
    "acq_EI = AcqEI(xi=0.)  # feel free to change the value of xi to see the different behaviour of EI\n",
    "\n",
    "# Calculate the acquisition function\n",
    "y_acq = acq_EI.get_acq(x.reshape(-1,1), gpr)\n",
    "\n",
    "\n",
    "# Visualize the acquisition functions \n",
    "\n",
    "fig, axes = plt.subplots(2,1,figsize=(5,4),gridspec_kw={\"height_ratios\": [2,1]})\n",
    "ax1, ax2 = axes\n",
    "ax2.set_ylabel('EI')\n",
    "\n",
    "plot_gp_with_acq(gpr, x, y, x_samples, y_samples, y_acq, axes=axes, fig=fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UCB acquisition\n",
    "acq_UCB = AcqUCB(k=2)  # feel free to change the value of xi to see the different behaviour of EI\n",
    "\n",
    "# Calculate the acquisition function\n",
    "y_acq = acq_UCB.get_acq(x.reshape(-1,1), gpr)\n",
    "\n",
    "# Visualize the acquisition functions \n",
    "\n",
    "fig, axes = plt.subplots(2,1,figsize=(5,4),gridspec_kw={\"height_ratios\": [2,1]})\n",
    "ax1, ax2 = axes\n",
    "ax2.set_ylabel('UCB')\n",
    "\n",
    "plot_gp_with_acq(gpr, x, y, x_samples, y_samples, y_acq, axes=axes, fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Maximize the Target Function\n",
    "\n",
    "_Note_: Here the code is used to maximize a unknown target fucntion. For minimization tasks, one can simply multiply -1 to the target function.\n",
    "\n",
    "Some helper functions are imported to plot the progress of BO, see `utils/gp_helper.py`\n",
    "\n",
    "The most simple form of Bayesian optimization can be divided in following steps:\n",
    "\n",
    "1. Initialize GP model $\\mathcal{GP}(\\mu, \\sigma)$\n",
    "2. Build acquisition function $\\alpha$\n",
    "3. Sample next point $x_i$ of target function $f$ at $\\text{argmax}(\\alpha)$\n",
    "4. Observe $y_i = f(x_{i})$ and refit GP model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Bayesian optimizer  \n",
    "\n",
    "def bayesian_optimize(gpmodel: GaussianProcessRegressor, acquisition: Acquisition, target_func, init_points=None, steps=50, bounds=None, xdim=1, rng=np.random.default_rng()):\n",
    "    # for logging purpose\n",
    "    history = {\n",
    "        \"X_init\": [],\n",
    "        \"Y_init\": [],\n",
    "        \"X\": [],\n",
    "        \"Y\": [],\n",
    "        \"X_best\": [],\n",
    "        \"Y_best\": [],\n",
    "        }\n",
    "\n",
    "    # Initial samples to start with \n",
    "    if init_points is None:\n",
    "        init_points = rng.uniform(low=bounds[:, 0], high=bounds[:, 1], size=(3, xdim))\n",
    "    y_init = []\n",
    "    for xi in init_points:\n",
    "        y_init.append(target_func(xi))\n",
    "    y_init = np.array(y_init)\n",
    "    history[\"X_init\"] = init_points\n",
    "    history[\"Y_init\"] = y_init\n",
    "    n_init = init_points.shape[0]\n",
    "\n",
    "    X = init_points.reshape(-1,xdim)\n",
    "    Y = y_init.reshape(-1,1)\n",
    "    \n",
    "    # Actual optimization step\n",
    "    for _ in range(steps):\n",
    "        # fit gp model\n",
    "        gpmodel.fit(X, Y)\n",
    "        # maximize acquisition\n",
    "        x_next = acquisition.suggest_next_sample(gpmodel, bounds=bounds)\n",
    "        # sample at argmax(acquisition)\n",
    "        y_next = target_func(x_next)\n",
    "        # augment data\n",
    "        X = np.vstack([X, x_next.reshape(-1,xdim)])\n",
    "        Y = np.vstack([Y, y_next.reshape(-1,1)])\n",
    "    \n",
    "    # log process\n",
    "    history[\"X\"] = X[n_init:]\n",
    "    history[\"Y\"] = Y[n_init:]\n",
    "    i_best = Y.argmax()\n",
    "    history[\"X_best\"] = X[i_best]\n",
    "    history[\"Y_best\"] = Y[i_best]\n",
    "    \n",
    "    return gpmodel, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Optimization [Feel free to play around with the parameters in this block and observe effects]\n",
    "\n",
    "# define a GP model\n",
    "kernel = 1 * RBF(length_scale=0.5, length_scale_bounds=[1e-2,1]) + WhiteKernel(noise_level=0.1) # allow automatic determination of length-scale\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "# define a Acquisition function\n",
    "myacq = AcqEI(xi=0.001)\n",
    "# start BO\n",
    "bounds = np.array([[0,1]])\n",
    "nsteps=30\n",
    "gpr, history = bayesian_optimize(gpr, myacq, target_func=f_target, steps=nsteps, bounds=bounds, rng=rng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Results\n",
    "fig, ax = plt.subplots()\n",
    "x_fine = np.linspace(0,1,500)\n",
    "y_fine = f_target(x_fine,noise=0)\n",
    "ymax = np.ones(nsteps)*y_fine.max()\n",
    "ax.plot(ymax, ls='--',color='gray', label='True maximum')\n",
    "ax.fill_between(np.arange(nsteps), ymax-0.1, ymax+0.1, alpha=0.3, color='gray')\n",
    "ax.plot(history[\"Y\"], label=\"BO result\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance benchmark (Optional Module)\n",
    "\n",
    "against \"classical\" algorithms:\n",
    "- Nelder-Mead\n",
    "- Newtonian methods\n",
    "\n",
    "### Define a more complex function\n",
    "\n",
    "Let's use the Ackley function:\n",
    "$$\n",
    "    f(x_1,x_2) = -20 \\exp \\left[-0.2 \\sqrt{0.5(x_1^2+x_2^2)} \\right] - \\exp \\left[0.5(\\cos(2\\pi x_1) + \\cos(2\\pi x_2)) \\right] + e + 20\n",
    "$$\n",
    "\n",
    "![Ackley function](img/ackley.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define more complex functions\n",
    "def f_ackley(x: np.ndarray, yhist = None) -> np.ndarray:\n",
    "    \"\"\"func_log: used to track the optimization progress\"\"\"\n",
    "    assert x.shape[1] ==2\n",
    "    y = -20 * np.exp(-0.2 * np.sqrt(0.5 * np.sum(np.square(x), axis=1))) \n",
    "    y -= np.exp(0.5*(np.cos(2*np.pi*x[:,0]) + np.cos(2*np.pi*x[:,1])))\n",
    "    y += np.e + 20\n",
    "    y *= -1\n",
    "    if yhist is not None:\n",
    "        yhist.append(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start optimization with different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_ackley = np.array([[-5,5],[-5,5]])\n",
    "n_restart = 10  # try 10 times for each algorithm\n",
    "maxiter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nelder Mead\n",
    "nm_hist = [0] * n_restart\n",
    "for i in range(n_restart):\n",
    "    rng = np.random.default_rng(i)\n",
    "    x0 = rng.uniform(low=bounds_ackley[:,0], high=bounds_ackley[:,1])\n",
    "    nm_hist[i] = []\n",
    "    res = minimize(\n",
    "        lambda x: -1 * float(f_ackley(np.array(x).reshape(-1,2), nm_hist[i])),\n",
    "        x0 = x0,\n",
    "        bounds=bounds_ackley,\n",
    "        options={'maxfev': maxiter, 'xatol': 1e-10},\n",
    "        method='Nelder-Mead'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# BO our version: more exploitation\n",
    "bo_hist_exploit = [0] * n_restart\n",
    "kernel = 1 * RBF(length_scale=0.3, length_scale_bounds=[1e-2,1]) #\n",
    "for i in range(n_restart):\n",
    "    rng = np.random.default_rng(i)\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "    myacq = AcqUCB(k=0.1)\n",
    "    bo_hist_exploit[i] = []\n",
    "    _, _ = bayesian_optimize(\n",
    "        gpr, myacq, target_func=lambda x: -1 * f_ackley(np.array(x).reshape(-1,2), bo_hist_exploit[i]), steps=maxiter, bounds=bounds_ackley, rng=rng, xdim=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BO our version: more exploration\n",
    "bo_hist_explore = [0] * n_restart\n",
    "kernel = 1 * RBF(length_scale=0.3, length_scale_bounds=[1e-2,1]) #\n",
    "for i in range(n_restart):\n",
    "    rng = np.random.default_rng(i)\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "    myacq = AcqUCB(k=2.0)\n",
    "    bo_hist_explore[i] = []\n",
    "    _, _ = bayesian_optimize(\n",
    "        gpr, myacq, target_func=lambda x: -1 * f_ackley(np.array(x).reshape(-1,2), bo_hist_explore[i]), steps=maxiter, bounds=bounds_ackley, rng=rng, xdim=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture  \n",
    "# suppress output\n",
    "\n",
    "# Default sklearn version of BO\n",
    "skopt_hist = [0] * n_restart\n",
    "for i in range(n_restart):\n",
    "    skopt_hist[i] = []\n",
    "    res = gp_minimize(lambda x: -1 * float(f_ackley(np.array(x).reshape(-1,2), skopt_hist[i])),dimensions=bounds_ackley,n_calls=maxiter, random_state=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig,ax = plt.subplots()\n",
    "plot_bo_result(nm_hist, ax, label='Nelder-Mead')\n",
    "plot_bo_result(skopt_hist, ax, label='scikit-BO')\n",
    "plot_bo_result(bo_hist_explore, ax, label='BO explore')\n",
    "plot_bo_result(bo_hist_exploit, ax, label='BO exploit')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## What's next?\n",
    "\n",
    "Hopefully you have learned something about BO, if you want to try it yourself afterwards, below are some interesting resources.\n",
    "\n",
    "\n",
    "### Publication: Various applications of BO in accelerator physics\n",
    "\n",
    "- Applications in LPA\n",
    "\n",
    "### Books and papers on Bayesian optimization in general\n",
    "\n",
    "- C. E. Rasmussen and C. K.I. Williams [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/): __the__ classic textbook of Gaussian process\n",
    "- \n",
    "\n",
    "\n",
    "### Bayesian Optimization / Gaussian Process packages in python\n",
    "\n",
    "Below is a incomplete selection of python packages for BO and GP, each with its own strength and drawback.\n",
    "\n",
    "- [scikit-learn Gaussian processes](https://scikit-learn.org/stable/modules/gaussian_process.html#) : recommended for sklearn users, not as powerful as other packages.\n",
    "- [GPyTorch](https://gpytorch.ai/) : a rather new package implemented natively in PyTorch, which makes it very performant. Also comes with a Bayesian optimization package [BOTorch](https://botorch.org/), offering a variety of different optimization methods (mult-objective, parallelization...). Both packages are being actively developed maintained; recommended for PyTorch users.\n",
    "- [GPflow](https://www.gpflow.org/) : a GP package implemented in TensorFlow, it also has a large community and is being actively maintained; The new BO package [Trieste](https://secondmind-labs.github.io/trieste) is built on it.\n",
    "- [GPy](http://sheffieldml.github.io/GPy/) from the Sheffield ML group : A common/classic choice for building GP model, includes a lot of advanced GP variants; However in recent years it is not so actively maintained. It comes with the accompanying Bayesian optimization package [GPyOpt](https://github.com/SheffieldML/GPyOpt), for which the maintainance stoped since 2020.\n",
    "- [Dragonfly] : a open-source BO package; offers also command line tool, easy to use if you are a practitioner. However if one has less freedom to adapt and expand the code.\n",
    "\n",
    "C.f. the [wikipedia page](https://en.wikipedia.org/wiki/Comparison_of_Gaussian_process_software#Comparison_table) for a more inclusive table with GP packages for other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('mltutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00213bb31e5d529a04e2295f28e6c984d5fc3e6a31e23b689ee98d3112623d68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
