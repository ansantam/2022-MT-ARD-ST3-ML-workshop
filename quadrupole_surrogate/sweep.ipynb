{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing ocelot...\n"
     ]
    }
   ],
   "source": [
    "import cheetah\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: c1m1k9ju\n",
      "Sweep URL: https://wandb.ai/msk-ipc/uncategorized/sweeps/c1m1k9ju\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"my-sweep\",\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"n_samples\": {\n",
    "            \"values\": [300, 5000, 10000]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [42, 200, 1000]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"min\": 0.0001,\n",
    "            \"max\": 0.1\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [32, 64, 128]\n",
    "        },\n",
    "        \"layer_width\": {\n",
    "            \"values\": [11, 32, 400]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"quadrupole-surrogate\", entity=\"msk-ipc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmsk-ipc\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jankaiser/Documents/DESY/quadrupole-surrogate/wandb/run-20220329_155747-2g6gm1z2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/msk-ipc/quadrupole-surrogate/runs/2g6gm1z2\" target=\"_blank\">twilight-snowball-2</a></strong> to <a href=\"https://wandb.ai/msk-ipc/quadrupole-surrogate\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"quadrupole-surrogate\",\n",
    "    entity=\"msk-ipc\"\n",
    ")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate input dataset\n",
    "Each sample is array of beam parameters, k1 and length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples):\n",
    "    parameter_keys = ['energy','mu_x','mu_xp','mu_y','mu_yp','sigma_p','sigma_s','sigma_x','sigma_xp','sigma_y','sigma_yp']\n",
    "\n",
    "    # Input data\n",
    "    parameters = [{\n",
    "        \"mu_x\": np.random.uniform(-1e-3, 1e-3),\n",
    "        \"mu_y\": np.random.uniform(-1e-3, 1e-3),\n",
    "        \"mu_xp\": np.random.uniform(-1e-4, 1e-4),\n",
    "        \"mu_yp\": np.random.uniform(-1e-4, 1e-4),\n",
    "        \"sigma_x\": np.random.uniform(1e-5, 5e-4),\n",
    "        \"sigma_y\": np.random.uniform(1e-5, 5e-4),\n",
    "        \"sigma_xp\": np.random.uniform(1e-6, 5e-5),\n",
    "        \"sigma_yp\": np.random.uniform(1e-6, 5e-5),\n",
    "        \"sigma_s\": np.random.uniform(1e-6, 5e-5),\n",
    "        \"sigma_p\": np.random.uniform(1e-4, 1e-3),\n",
    "        \"energy\": np.random.uniform(80e6, 160e6)\n",
    "    } for _ in range(n_samples)]\n",
    "\n",
    "    beams = [cheetah.ParameterBeam.from_parameters(**p) for p in parameters]\n",
    "\n",
    "    X1 = np.array([[b.parameters[k] for k in parameter_keys] for b in beams])\n",
    "\n",
    "    X0 = np.array([[np.random.uniform(0.1, 0.3), np.random.uniform(-15.0, 15.0)] for _ in range(n_samples)])\n",
    "\n",
    "    X = np.hstack([X0, X1])\n",
    "\n",
    "    # Output data\n",
    "    y = []\n",
    "    for incoming, x in zip(beams, X0):\n",
    "        quadrupole = cheetah.Quadrupole(length=x[0], k1=x[1])\n",
    "        outgoing = quadrupole(incoming)\n",
    "        y.append(outgoing)\n",
    "\n",
    "    y = np.array([[b.parameters[k] for k in parameter_keys] for b in y])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ANN\n",
    "Not comment needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    with wandb.init(project=\"quadrupole-surrogate\", entity=\"msk-ipc\") as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        # Prepare data\n",
    "        X, y = generate_data(config.n_samples)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
    "        X_scaler = StandardScaler()\n",
    "        X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = X_scaler.transform(X_test)\n",
    "        y_scaler = StandardScaler()\n",
    "        y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "        y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "        # Make model\n",
    "        model = keras.Sequential([\n",
    "            Dense(config.layer_width, activation=\"relu\"),\n",
    "            Dense(config.layer_width, activation=\"relu\"),\n",
    "            Dense(11)\n",
    "        ])\n",
    "        model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "        # Train\n",
    "        model.fit(\n",
    "            X_train_scaled,\n",
    "            y_train_scaled,\n",
    "            batch_size=config.batch_size,\n",
    "            epochs=config.epochs,\n",
    "            validation_data=(X_test_scaled,y_test_scaled),\n",
    "            callbacks=[WandbCallback()]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b4gf89j7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_width: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.016561578871106257\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_samples: 300\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jankaiser/Documents/DESY/quadrupole-surrogate/wandb/run-20220329_155842-b4gf89j7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/msk-ipc/uncategorized/runs/b4gf89j7\" target=\"_blank\">colorful-sweep-1</a></strong> to <a href=\"https://wandb.ai/msk-ipc/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/msk-ipc/uncategorized/sweeps/c1m1k9ju\" target=\"_blank\">https://wandb.ai/msk-ipc/uncategorized/sweeps/c1m1k9ju</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 15:58:53.726147: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2/2 [==============================] - 7s 2s/step - loss: 1.1368 - mae: 0.8672 - val_loss: 1.1536 - val_mae: 0.8562 - _timestamp: 1648562350.0000 - _runtime: 28.0000\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 1.1203 - mae: 0.8608 - val_loss: 1.1429 - val_mae: 0.8519 - _timestamp: 1648562350.0000 - _runtime: 28.0000\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 311ms/step - loss: 1.1039 - mae: 0.8547 - val_loss: 1.1330 - val_mae: 0.8479 - _timestamp: 1648562351.0000 - _runtime: 29.0000\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 301ms/step - loss: 1.0900 - mae: 0.8493 - val_loss: 1.1236 - val_mae: 0.8442 - _timestamp: 1648562351.0000 - _runtime: 29.0000\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0763 - mae: 0.8440WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0073s). Check your callbacks.\n",
      "2/2 [==============================] - 1s 490ms/step - loss: 1.0763 - mae: 0.8440 - val_loss: 1.1149 - val_mae: 0.8409 - _timestamp: 1648562351.0000 - _runtime: 29.0000\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 1.0644 - mae: 0.8393 - val_loss: 1.1068 - val_mae: 0.8378 - _timestamp: 1648562352.0000 - _runtime: 30.0000\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 217ms/step - loss: 1.0523 - mae: 0.8346 - val_loss: 1.0993 - val_mae: 0.8348 - _timestamp: 1648562352.0000 - _runtime: 30.0000\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 277ms/step - loss: 1.0417 - mae: 0.8304 - val_loss: 1.0924 - val_mae: 0.8320 - _timestamp: 1648562352.0000 - _runtime: 30.0000\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 412ms/step - loss: 1.0320 - mae: 0.8265 - val_loss: 1.0859 - val_mae: 0.8294 - _timestamp: 1648562353.0000 - _runtime: 31.0000\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 402ms/step - loss: 1.0228 - mae: 0.8228 - val_loss: 1.0798 - val_mae: 0.8267 - _timestamp: 1648562353.0000 - _runtime: 31.0000\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 214ms/step - loss: 1.0145 - mae: 0.8194 - val_loss: 1.0739 - val_mae: 0.8243 - _timestamp: 1648562353.0000 - _runtime: 31.0000\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 287ms/step - loss: 1.0065 - mae: 0.8161 - val_loss: 1.0685 - val_mae: 0.8219 - _timestamp: 1648562354.0000 - _runtime: 32.0000\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 294ms/step - loss: 0.9994 - mae: 0.8132 - val_loss: 1.0634 - val_mae: 0.8197 - _timestamp: 1648562354.0000 - _runtime: 32.0000\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 0.9927 - mae: 0.8103 - val_loss: 1.0587 - val_mae: 0.8175 - _timestamp: 1648562354.0000 - _runtime: 32.0000\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 234ms/step - loss: 0.9862 - mae: 0.8075 - val_loss: 1.0542 - val_mae: 0.8155 - _timestamp: 1648562354.0000 - _runtime: 32.0000\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 197ms/step - loss: 0.9810 - mae: 0.8052 - val_loss: 1.0500 - val_mae: 0.8135 - _timestamp: 1648562355.0000 - _runtime: 33.0000\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 378ms/step - loss: 0.9753 - mae: 0.8027 - val_loss: 1.0461 - val_mae: 0.8117 - _timestamp: 1648562355.0000 - _runtime: 33.0000\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 281ms/step - loss: 0.9700 - mae: 0.8003 - val_loss: 1.0423 - val_mae: 0.8099 - _timestamp: 1648562355.0000 - _runtime: 33.0000\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 311ms/step - loss: 0.9654 - mae: 0.7984 - val_loss: 1.0387 - val_mae: 0.8082 - _timestamp: 1648562356.0000 - _runtime: 34.0000\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 218ms/step - loss: 0.9608 - mae: 0.7963 - val_loss: 1.0353 - val_mae: 0.8065 - _timestamp: 1648562356.0000 - _runtime: 34.0000\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.9565 - mae: 0.7945 - val_loss: 1.0320 - val_mae: 0.8050 - _timestamp: 1648562356.0000 - _runtime: 34.0000\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 371ms/step - loss: 0.9525 - mae: 0.7927 - val_loss: 1.0287 - val_mae: 0.8034 - _timestamp: 1648562357.0000 - _runtime: 35.0000\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 1s 585ms/step - loss: 0.9486 - mae: 0.7911 - val_loss: 1.0255 - val_mae: 0.8019 - _timestamp: 1648562357.0000 - _runtime: 35.0000\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 1s 651ms/step - loss: 0.9448 - mae: 0.7895 - val_loss: 1.0224 - val_mae: 0.8005 - _timestamp: 1648562358.0000 - _runtime: 36.0000\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 361ms/step - loss: 0.9412 - mae: 0.7880 - val_loss: 1.0194 - val_mae: 0.7991 - _timestamp: 1648562359.0000 - _runtime: 37.0000\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 425ms/step - loss: 0.9378 - mae: 0.7866 - val_loss: 1.0165 - val_mae: 0.7976 - _timestamp: 1648562359.0000 - _runtime: 37.0000\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 333ms/step - loss: 0.9344 - mae: 0.7851 - val_loss: 1.0137 - val_mae: 0.7962 - _timestamp: 1648562359.0000 - _runtime: 37.0000\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 232ms/step - loss: 0.9310 - mae: 0.7837 - val_loss: 1.0109 - val_mae: 0.7948 - _timestamp: 1648562360.0000 - _runtime: 38.0000\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 476ms/step - loss: 0.9278 - mae: 0.7823 - val_loss: 1.0081 - val_mae: 0.7935 - _timestamp: 1648562360.0000 - _runtime: 38.0000\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 271ms/step - loss: 0.9245 - mae: 0.7809 - val_loss: 1.0054 - val_mae: 0.7921 - _timestamp: 1648562360.0000 - _runtime: 38.0000\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 348ms/step - loss: 0.9211 - mae: 0.7794 - val_loss: 1.0028 - val_mae: 0.7908 - _timestamp: 1648562361.0000 - _runtime: 39.0000\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 295ms/step - loss: 0.9181 - mae: 0.7781 - val_loss: 1.0001 - val_mae: 0.7895 - _timestamp: 1648562361.0000 - _runtime: 39.0000\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 215ms/step - loss: 0.9148 - mae: 0.7767 - val_loss: 0.9975 - val_mae: 0.7882 - _timestamp: 1648562361.0000 - _runtime: 39.0000\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 321ms/step - loss: 0.9115 - mae: 0.7752 - val_loss: 0.9948 - val_mae: 0.7868 - _timestamp: 1648562362.0000 - _runtime: 40.0000\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 223ms/step - loss: 0.9084 - mae: 0.7738 - val_loss: 0.9921 - val_mae: 0.7855 - _timestamp: 1648562362.0000 - _runtime: 40.0000\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 335ms/step - loss: 0.9052 - mae: 0.7723 - val_loss: 0.9895 - val_mae: 0.7841 - _timestamp: 1648562362.0000 - _runtime: 40.0000\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 245ms/step - loss: 0.9019 - mae: 0.7708 - val_loss: 0.9870 - val_mae: 0.7828 - _timestamp: 1648562363.0000 - _runtime: 41.0000\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 235ms/step - loss: 0.8988 - mae: 0.7694 - val_loss: 0.9843 - val_mae: 0.7815 - _timestamp: 1648562363.0000 - _runtime: 41.0000\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 373ms/step - loss: 0.8957 - mae: 0.7679 - val_loss: 0.9817 - val_mae: 0.7803 - _timestamp: 1648562363.0000 - _runtime: 41.0000\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 213ms/step - loss: 0.8926 - mae: 0.7664 - val_loss: 0.9791 - val_mae: 0.7790 - _timestamp: 1648562363.0000 - _runtime: 41.0000\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 332ms/step - loss: 0.8894 - mae: 0.7649 - val_loss: 0.9766 - val_mae: 0.7777 - _timestamp: 1648562364.0000 - _runtime: 42.0000\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 215ms/step - loss: 0.8863 - mae: 0.7634 - val_loss: 0.9741 - val_mae: 0.7764 - _timestamp: 1648562364.0000 - _runtime: 42.0000\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 224ms/step - loss: 0.8831 - mae: 0.7619 - val_loss: 0.9715 - val_mae: 0.7751 - _timestamp: 1648562364.0000 - _runtime: 42.0000\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 207ms/step - loss: 0.8800 - mae: 0.7603 - val_loss: 0.9690 - val_mae: 0.7738 - _timestamp: 1648562365.0000 - _runtime: 43.0000\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 278ms/step - loss: 0.8769 - mae: 0.7587 - val_loss: 0.9665 - val_mae: 0.7725 - _timestamp: 1648562365.0000 - _runtime: 43.0000\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.8738 - mae: 0.7572 - val_loss: 0.9640 - val_mae: 0.7712 - _timestamp: 1648562365.0000 - _runtime: 43.0000\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 200ms/step - loss: 0.8706 - mae: 0.7557 - val_loss: 0.9615 - val_mae: 0.7699 - _timestamp: 1648562365.0000 - _runtime: 43.0000\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 242ms/step - loss: 0.8676 - mae: 0.7542 - val_loss: 0.9591 - val_mae: 0.7687 - _timestamp: 1648562365.0000 - _runtime: 43.0000\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 1s 650ms/step - loss: 0.8645 - mae: 0.7526 - val_loss: 0.9567 - val_mae: 0.7675 - _timestamp: 1648562366.0000 - _runtime: 44.0000\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 1s 508ms/step - loss: 0.8614 - mae: 0.7511 - val_loss: 0.9543 - val_mae: 0.7663 - _timestamp: 1648562366.0000 - _runtime: 44.0000\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 327ms/step - loss: 0.8583 - mae: 0.7496 - val_loss: 0.9519 - val_mae: 0.7651 - _timestamp: 1648562367.0000 - _runtime: 45.0000\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 322ms/step - loss: 0.8552 - mae: 0.7482 - val_loss: 0.9496 - val_mae: 0.7640 - _timestamp: 1648562367.0000 - _runtime: 45.0000\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 255ms/step - loss: 0.8522 - mae: 0.7467 - val_loss: 0.9472 - val_mae: 0.7628 - _timestamp: 1648562367.0000 - _runtime: 45.0000\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 335ms/step - loss: 0.8492 - mae: 0.7452 - val_loss: 0.9449 - val_mae: 0.7617 - _timestamp: 1648562368.0000 - _runtime: 46.0000\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 305ms/step - loss: 0.8462 - mae: 0.7437 - val_loss: 0.9425 - val_mae: 0.7606 - _timestamp: 1648562368.0000 - _runtime: 46.0000\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 191ms/step - loss: 0.8430 - mae: 0.7422 - val_loss: 0.9402 - val_mae: 0.7594 - _timestamp: 1648562369.0000 - _runtime: 47.0000\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 294ms/step - loss: 0.8400 - mae: 0.7407 - val_loss: 0.9378 - val_mae: 0.7583 - _timestamp: 1648562369.0000 - _runtime: 47.0000\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 194ms/step - loss: 0.8369 - mae: 0.7391 - val_loss: 0.9354 - val_mae: 0.7573 - _timestamp: 1648562369.0000 - _runtime: 47.0000\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 292ms/step - loss: 0.8339 - mae: 0.7376 - val_loss: 0.9331 - val_mae: 0.7562 - _timestamp: 1648562369.0000 - _runtime: 47.0000\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 296ms/step - loss: 0.8307 - mae: 0.7361 - val_loss: 0.9308 - val_mae: 0.7552 - _timestamp: 1648562370.0000 - _runtime: 48.0000\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 1s 536ms/step - loss: 0.8277 - mae: 0.7345 - val_loss: 0.9285 - val_mae: 0.7541 - _timestamp: 1648562370.0000 - _runtime: 48.0000\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 475ms/step - loss: 0.8246 - mae: 0.7330 - val_loss: 0.9261 - val_mae: 0.7531 - _timestamp: 1648562371.0000 - _runtime: 49.0000\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.8215 - mae: 0.7315 - val_loss: 0.9238 - val_mae: 0.7521 - _timestamp: 1648562371.0000 - _runtime: 49.0000\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 412ms/step - loss: 0.8186 - mae: 0.7299 - val_loss: 0.9216 - val_mae: 0.7511 - _timestamp: 1648562371.0000 - _runtime: 49.0000\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 298ms/step - loss: 0.8155 - mae: 0.7284 - val_loss: 0.9193 - val_mae: 0.7501 - _timestamp: 1648562372.0000 - _runtime: 50.0000\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 252ms/step - loss: 0.8126 - mae: 0.7268 - val_loss: 0.9171 - val_mae: 0.7491 - _timestamp: 1648562372.0000 - _runtime: 50.0000\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 315ms/step - loss: 0.8096 - mae: 0.7253 - val_loss: 0.9148 - val_mae: 0.7482 - _timestamp: 1648562372.0000 - _runtime: 50.0000\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 0.8067 - mae: 0.7237 - val_loss: 0.9126 - val_mae: 0.7472 - _timestamp: 1648562373.0000 - _runtime: 51.0000\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 259ms/step - loss: 0.8036 - mae: 0.7221 - val_loss: 0.9103 - val_mae: 0.7462 - _timestamp: 1648562373.0000 - _runtime: 51.0000\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05cbf078d01219c592fed14fc536999284c9d9d80b38ec1bed3cfe813d108552"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('surrogate-tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
